
# Inferential statistics

In our models, we focussed on a contrast difference of .25 vs. .75. This is because the 0 and 1 contrast difference conditions were different from the other contrast levels: At 0 contrast difference, there was no correct answer and feedback was probabilistic, making those trials hard to compare with the others. At a contrast difference of 1, there was only one stimulus presented, potentially changing the (visual) neural response, introducing a bias to our models which are based on spiking data. Hence, consistently with our models, we compared the behavioural data between a contrast difference of .25 vs. .75 only.

```{r contrast_25_75}
# Reduce data to go trials with an absolute contrast difference of .25 and .75
data_25_75 <- data_wof %>% 
  filter(condition == "go", contrast_diff_abs %in% c(.25, .75))
```


## Response times
```{r response_time_25_vs_75}
data_25_75 %>% 
  
  ggplot(aes(x = factor(contrast_diff_abs), y = response_time_diff, colour = feedback_type_rec)) +
  geom_jitter(alpha = .4) +
  stat_summary(aes(fill = feedback_type_rec), size = 1, shape = 21, colour = "black") +
  theme(legend.position = "top") +
  facet_wrap(~response_rec2) +
  scale_colour_manual("feedback", values = c("darkred", "lightblue3"), labels = c("wrong", "correct")) +
  scale_fill_manual("feedback", values = c("darkred", "lightblue3"), labels = c("wrong", "correct")) +
  labs(title = "Go trial response times", subtitle = "by correct response and contrast difference",
       y = "response time from go cue", x = "absolute contrast difference") +
  my_theme
```

Obviously, response times are different for no-go responses. Also, a no-go response is always wrong. We hence set up a model for go trials where the mice did not make a no-go response with the following predictors: Contrast level (.25 or .75, coded as -.5 and .5) and feedback (wrong or correct, coded as -.5 and .5). We added a random intercept for each mouse.

```{r mm_response_time}
# TO DO: FIX THAT P-VALUES ARE CUT OFF!

data_response_time <- data_25_75 %>% 
  filter(response_rec2 == "other") %>% # filter out no-go responses
  # Recode predictors for mixed model
  mutate(contrast_diff_eff = ifelse(contrast_diff_abs == .25, -.5, .5),
         feedback_eff = ifelse(feedback_type_rec == "punishment", -.5, .5))

mm_response_time <- lmer(response_time_diff ~ contrast_diff_eff * feedback_eff + (1|mouse),
                         data = data_response_time)

tidy(mm_response_time) %>% 
  mutate(p.value = gsub("\\$", "", format_p(p.value, numbers_only = TRUE))) %>% 
  mutate_if(is.numeric, list(~round(., 3))) %>% 
  mutate(term = ifelse(grepl("sd_", term), paste0("sd_", group), term)) %>% 
  select(-group) %>% 
  kable(align = c(rep("l", 2), rep("r", 5))) %>% 
  kable_styling("striped")
```

It seems like overall (i.e. at the mean of correct and wrong responses), there is no effect of contrast difference. There is, however, an effect of feedback: Wrong responses are slower than correct ones (at the mean of a contrast difference .25 and .75). Lastly, there is a strong contrast x feedback interaction: Response times are slower for wrong responses, but only contrast difference of .75.

We run a follow up-model to check for simple effects by changing the coding from -.5 and .5 to 0 and 1. Here, we can confirm that there is a difference between .25 and .75 for wrong responses (which are coded as 0 here). Furthermore, there is an effect of feedback (correct vs. wrong) at a contrast difference of .25 already.

```{r mm_response_time_dummy}
data_response_time <- data_25_75 %>% 
  filter(response_rec2 == "other") %>% # filter out no-go responses
  # Recode predictors for mixed model
  mutate(contrast_diff_eff = ifelse(contrast_diff_abs == .25, 0, 1),
         feedback_eff = ifelse(feedback_type_rec == "punishment", 0, 1))

mm_response_time <- lmer(response_time_diff ~ contrast_diff_eff * feedback_eff + (1|mouse),
                         data = data_response_time)

tidy(mm_response_time) %>% 
  mutate(p.value = gsub("\\$", "", format_p(p.value, numbers_only = TRUE))) %>% 
  mutate_if(is.numeric, list(~round(., 3))) %>% 
  mutate(term = ifelse(grepl("sd_", term), paste0("sd_", group), term)) %>% 
  select(-group) %>% 
  kable(align = c(rep("l", 2), rep("r", 5))) %>% 
  kable_styling("striped")
```

### Relation to the drift diffusion model
The drift diffusion model tells us that the drift rate is higher for the easier trials. At first, this doesn't seem to fit with the slower reaction times in the easy condition. However, it appears that if the drift rate varies across trials, it is not only possible that reaction times differ, but also that error reaction times are slower than correct reaction times. This is the case when e.g. accuracy is stressed in the task instructions [@ratcliff_2008]. In our case, this fits because the mice are really good at the task. They seem to be motivated to get it right, especially in the easier trials (because they know they can do it?). This is in line with the fact that there are less incorrect responses over time (but instead, more no-go responses).

## Performance
Is the proportion of no-go vs. correct vs. incorrect responses different between the contrasts?

```{r}
# Table of trial counts for no-go, correct and incorrect by contrast
response_table <- data_25_75 %>% 
  mutate(response_type = case_when(response_rec2 == "no go" ~ "no go",
                                   response_rec2 == "other" & feedback_type == 1 ~ "correct",
                                   response_rec2 == "other" & feedback_type == -1 ~ "wrong")) %>% 
  group_by(contrast_diff_abs, response_type) %>% 
  count()

response_table_wide <- response_table %>% 
  pivot_wider(names_from = contrast_diff_abs, values_from = n, names_prefix = "contr_")

response_table_wide %>% 
  kable(col.names = c("response", "0.25", "0.75")) %>% 
  kable_styling("striped")
```

```{r}
# Chi square for proportions
chisq_response <- chisq.test(response_table_wide[2:3])
```

The proportion of correct, no-go and wrong responses seems to be different between a contrast of .25 and .75, `r print_chi2(chisq_response)`. Descriptively, there is overall the same amount of errors for the higher contrast, but those are more likely to be no-go responses than wrong responses.

```{r}
response_table %>% 
  group_by(contrast_diff_abs) %>% 
  mutate(percent = n/sum(n)) %>% 
  pivot_longer(cols = c(n, percent), names_to = "type", values_to = "value") %>% 
  mutate(response_type = factor(response_type, levels = c("no go", "wrong", "correct"))) %>% 
  
  ggplot(aes(x = factor(contrast_diff_abs), y = value, fill = response_type)) +
  geom_col() +
  scale_fill_manual("response", values = c("darkgrey", "darkred", "darkgreen")) +
  facet_wrap(~type, scales = "free") +
  labs(title = "Performance", subtitle = "as a function of contrast difference",
       y = "", x = "absolute contrast difference") +
  my_theme
```

## References
